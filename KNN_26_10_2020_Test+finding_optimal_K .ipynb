{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is what i add to this file remove dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dates(text):\n",
    "    \n",
    "    months=['Janvier','Fevrier','Mars','Avril','Mai','Juin','Juillet','Aout','Août','Septembre','Octobre','Novembre','Décembre']\n",
    "    for month in months:\n",
    "        text = text.replace(month.lower(), '')\n",
    "        text = text.replace(month, '')\n",
    "    \n",
    "    dates = None\n",
    "    try:\n",
    "        pattern = re.compile(r'[0-9][0-9]+')\n",
    "        dates = pattern.findall(text)\n",
    "        for date in dates:\n",
    "            text = text.replace(date, '')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    dates\n",
    "\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter  \n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO \n",
    "#from docx import Document\n",
    "\n",
    "\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'ascii'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec='ascii', laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password, caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    str = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return str \n",
    "\n",
    "def read_file(fileName):\n",
    "  extension = fileName.split(\".\")\n",
    "  extension = extension.pop()\n",
    "  if extension == \"pdf\":\n",
    "    #try:\n",
    "       return convert_pdf_to_txt(fileName) #call to the conversion function pdftotxt\n",
    "    #except:\n",
    "       #return ''\n",
    "       #pass\n",
    "  else:\n",
    "    print ('Unsupported format')\n",
    "  return '', ''\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['education', 'formations', 'formation', 'diplômes', 'diplomes', 'formations académiques', 'formations academiques', 'formation et diplômes', 'formation et diplomes', 'diplômes et formations', 'diplomes et formations', 'diplômes & formations', 'diplomes & formations', 'parcours de formation', 'études et diplômes', 'etudes et diplomes', 'logiciels', 'qualités', 'qualites', 'qualites', 'informatique', 'outil informatique', 'expériences professionnelles', 'experiences professionnelles', 'expériences', 'experiences', 'expériences pro', 'experiences pro', 'projets & expériences professionnelles', 'projets & experiences professionnelles', 'expérience', 'experience', 'experience professionnelle', 'experience associative', 'experiences techniques', 'expérience', 'experience', 'compétences', 'competences', 'compétences technique', 'competences technique', 'compétences informatiques', 'competences informatiques', 'compétences linguistiques', 'competences linguistiques', 'compétences linguistiques & informatiques', 'competences linguistiques & informatiques', 'connaissances', 'compétences et connaissances', 'competences et connaissances', 'projets scolaires', 'projets', 'langues/ informatiques', 'langues', 'langues et informatique', 'langues et competences', 'informatiques', 'connaissances qualifications', 'experiences professionnelle', 'nom', 'e-mail', 'email', 'mail', 'mobile', 'telephone']\n",
      "['\\ufefflangues', 'langues', 'date de naissance', 'langue', 'coordonnées', 'coordonnees', 'divers', \"centres d'interet\", 'infos', 'contact', 'loisirs', 'présentation', 'presentation', 'profil', 'hobbies', 'me contacter', 'objectifs', 'site web', 'personnalité', 'personnalite', 'intérets', 'interets', 'réseaux sociaux', 'reseaux sociaux', 'une vie dynamique', 'objectif', 'informations complementaires', 'personnalité', 'personnalite', 'engagements personnels', \"centres d'intérets\", \"centres d'interets\", \"centre d'intérêts\", \"centre d'interets\"]\n"
     ]
    }
   ],
   "source": [
    "useful_entities = []\n",
    "filepath = 'titres_utiles.txt'\n",
    "with open(filepath, encoding='utf-8') as fp:\n",
    "   line = fp.readline()\n",
    "   cnt = 1\n",
    "   while line:\n",
    "       useful_entities.append(str.lower(line.strip()))\n",
    "       line = fp.readline()\n",
    "       cnt += 1\n",
    "print(useful_entities)\n",
    "\n",
    "\n",
    "unnecessary_entities = []\n",
    "filepath = 'titres_inutiles.txt'\n",
    "with open(filepath, encoding='utf-8') as fp:\n",
    "   line = fp.readline()\n",
    "   cnt = 1\n",
    "   while line:\n",
    "       unnecessary_entities.append(str.lower(line.strip()))\n",
    "       line = fp.readline()\n",
    "       cnt += 1\n",
    "print(unnecessary_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import nltk \n",
    "import codecs\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, line_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Tokenizing Words , lines and Sentences\n",
    "\n",
    "def get_lines(text):\n",
    "    return line_tokenize(text)\n",
    "def get_words(text):\n",
    "    return word_tokenize(text)\n",
    "def get_sentence(text):\n",
    "    return sent_tokenize(text)\n",
    "# remove punctuation and stop words french\n",
    "\n",
    "'''A stop word is a commonly used word (such as “un”, “j'ai”, “avec”, “et”) \n",
    "that a search engine has been programmed to ignore'''\n",
    "\n",
    "def removePunct(text_string):\n",
    "    #text_string = (text.encode('ascii', errors='ignore')).decode(\"utf-8\")\n",
    "    text_string=text_string.replace(\"\\n\",\" \")\n",
    "    text_string=text_string.replace(\"\\uf0b7\",\" \")\n",
    "    text_string=text_string.replace(\"\\x0c\",\" \")\n",
    "    text_string=text_string.replace(\"\\t\",\" \")\n",
    "    text_string=text_string.replace(\"\\x0b\",\" \")\n",
    "    text_string=text_string.replace(\"\\r\",\" \")\n",
    "    text_string=text_string.replace(\"\\f\",\" \")\n",
    "    \n",
    "    text_string=text_string.replace(\"’\",\" \")\n",
    "    \n",
    "    text_string=text_string.replace(\"\\u200b\",\" \")\n",
    "    text_string=text_string.replace(\"\\uf020\",\" \")\n",
    "    text_string=text_string.replace(\"\\uf02d\",\" \")\n",
    "     \n",
    "    \n",
    "    \n",
    "    for i in text_string:\n",
    "        if i in string.punctuation:\n",
    "            text_string=text_string.replace(i,\" \")\n",
    "    words = get_words(text_string.lower())\n",
    "    return ' '.join(words)\n",
    "def stopword(text) :\n",
    "    words = set(stopwords.words('french'))\n",
    "    word_tokens= [w for w in get_words(text) if not w in words]\n",
    "    return word_tokens\n",
    "\n",
    "#Stemming\n",
    "\n",
    "'''\n",
    "Stemming is the process of reducing a word into its stem, i.e. its root form.\n",
    "Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "\n",
    "'''\n",
    "def stemmers(text):\n",
    "    stemmer = SnowballStemmer(\"french\")\n",
    "    w=get_words(text)\n",
    "    w = [stemmer.stem(word) for word in w]\n",
    "    return ' '.join(w)\n",
    "#text net\n",
    "\n",
    "def net_text(text) :\n",
    "    doc = removePunct(text)\n",
    "    doc = ' '.join(stopword(doc))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# import the necessary libraries\n",
    "import os, re, sys\n",
    "\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "# from commun.process_text import *\n",
    "# diri=os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n",
    "# # open the file that contains the useful entities\n",
    "# useful_entities = open(diri+'/util/data/useful_entities.txt', 'r').read().lower()\n",
    "# # open the file that contains unnecessary entities\n",
    "# unnecessary_entities = open(diri+'/util/data/unnecessary_entities.txt', 'r').read().lower()\n",
    "\n",
    "\n",
    "# this function return a list of entity indexes\n",
    "\n",
    "def get_index(text):\n",
    "    lines = get_lines(text.lower())\n",
    "    all_index = []\n",
    "    for i in lines:\n",
    "        if len(i) >= 5:\n",
    "            index = lines.index(i)\n",
    "            if (i in useful_entities + unnecessary_entities) and (index not in all_index):\n",
    "                all_index.append(index)\n",
    "    return all_index\n",
    "\n",
    "\n",
    "# this function returns a matrix of two columns contains indeces that surround the unnecessary entities\n",
    "def get_other_index(text):\n",
    "    all_index = get_index(text)\n",
    "    lines = get_lines(text.lower())\n",
    "    index_other = []\n",
    "    for i in range(len(all_index) - 1):\n",
    "        m = all_index[i]\n",
    "        f = all_index[i + 1]\n",
    "        \n",
    "#         if lines[m] in unnecessary_entities.split(' , '):\n",
    "        if lines[m] in unnecessary_entities:\n",
    "            index_other.append([m, f])\n",
    "#         elif lines[m] in unnecessary_entities.split(', '):\n",
    "        elif lines[m] in unnecessary_entities:\n",
    "            index_other.append([m, f])\n",
    "#         elif lines[m] in unnecessary_entities.split(' ,'):\n",
    "        elif lines[m] in unnecessary_entities:\n",
    "            index_other.append([m, f])\n",
    "    return index_other\n",
    "\n",
    "\n",
    "# this function remove the unnecessary entities\n",
    "def remove_entities(text):\n",
    "    lines = get_lines(text.lower())\n",
    "    index_other = get_other_index(text)\n",
    "    for k in range(len(index_other)):\n",
    "        liste = index_other[k]\n",
    "        del lines[liste[0]:liste[1]]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def remove_email(text):\n",
    "    emails = None\n",
    "    try:\n",
    "        pattern = re.compile(r'\\S*@\\S*')\n",
    "        emails = pattern.findall(text)\n",
    "        for email in emails:\n",
    "            text = text.replace(email, '')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_dates(text):\n",
    "    \n",
    "    months=['Janvier','Fevrier','Mars','Avril','Mai','Juin','Juillet','Aout','Août','Septembre','Octobre','Novembre','Décembre']\n",
    "    for month in months:\n",
    "        text = text.replace(month.lower(), '')\n",
    "        text = text.replace(month, '')\n",
    "    \n",
    "    dates = None\n",
    "    try:\n",
    "        pattern = re.compile(r'[0-9][0-9]+')\n",
    "        dates = pattern.findall(text)\n",
    "        for date in dates:\n",
    "            text = text.replace(date, '')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    dates\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_phone(text):\n",
    "    number = None\n",
    "    try:\n",
    "        pattern = re.compile(\n",
    "            r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "        match = pattern.findall(text)\n",
    "        match = [re.sub(r'[,.]', '', el) for el in match if len(re.sub(r'[()\\-.,\\s+]', '', el)) > 6]\n",
    "        match = [re.sub(r'\\D$', '', el).strip() for el in match]\n",
    "        match = [el for el in match if len(re.sub(r'\\D', '', el)) <= 15]\n",
    "        try:\n",
    "            for el in list(match):\n",
    "                if len(el.split('-')) > 3: continue  # Year format YYYY-MM-DD\n",
    "                for x in el.split(\"-\"):\n",
    "                    try:\n",
    "                        if x.strip()[-4:].isdigit():\n",
    "                            if int(x.strip()[-4:]) in range(1900, 2100):\n",
    "                                match.remove(el)\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "        number = match\n",
    "        for n in number:\n",
    "            text = text.replace(n, '')\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "\n",
    "def ie_preprocess(document):\n",
    "    document = ' '.join(stopword(document))\n",
    "    sentences = get_sentence(document)\n",
    "    sentences = [get_words(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def remove_names(lines):\n",
    "    names = []\n",
    "    test = 1\n",
    "    text = ' '.join(lines)\n",
    "    with codecs.open('names.txt', 'r', encoding='utf8') as f:\n",
    "        names = f.read().lower().split(' , ')\n",
    "    try:\n",
    "        for i in lines:\n",
    "            words = word_tokenize(i.lower())\n",
    "            if len(words) <= 3:\n",
    "                for j in words:\n",
    "                    if len(j) >= 4:\n",
    "                        if j in names:\n",
    "                            lines.remove(i)\n",
    "                            text = ' '.join(lines)\n",
    "                            test = 0\n",
    "        if test == 1:\n",
    "            sentences = ie_preprocess(text)\n",
    "            for tagged_sentence in sentences:\n",
    "                for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "                    if type(chunk) == nltk.tree.Tree and chunk.label() == 'PERSON':\n",
    "                        names.append(' '.join([c[0] for c in chunk]))\n",
    "            for n in names:\n",
    "                text = text.replace(n, '')\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "#### to parse the name\n",
    "#in the original code the purpose was to delete name... I edit this function\n",
    "def find_name(text):\n",
    "    names = []\n",
    "    founded_name=''\n",
    "#     test = 1\n",
    "#     text = ' '.join(lines)\n",
    "    with codecs.open('names.txt', 'r', encoding='utf8') as f:\n",
    "        names = f.read().lower().split(' , ')\n",
    "    try:\n",
    "        for i in lines:\n",
    "            words = word_tokenize(i.lower())\n",
    "            if len(words) <= 3:\n",
    "                for j in words:\n",
    "                    if len(j) >= 4:\n",
    "                        if j in names:\n",
    "                            #founded_name=i\n",
    "                            lines.remove(i)\n",
    "                            text = ' '.join(lines)\n",
    "                            test = 0\n",
    "        if test == 1:\n",
    "            sentences = ie_preprocess(text)\n",
    "            for tagged_sentence in sentences:\n",
    "                for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "                    if type(chunk) == nltk.tree.Tree and chunk.label() == 'PERSON':\n",
    "                        names.append(' '.join([c[0] for c in chunk]))\n",
    "            for n in names:\n",
    "                text = text.replace(n, '')\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "def cv_cleaner(text):\n",
    "    lines = remove_entities(text)\n",
    "    doc = ' '.join(lines)\n",
    "    doc = remove_names(lines)\n",
    "    doc1 = remove_phone(doc)\n",
    "    doc2 = remove_email(doc1)\n",
    "    doc3=remove_dates(doc2)\n",
    "    w = net_text(doc3)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'formation cours cycle ingénieur etat a institut nationale statistique economie appliquée option recherche opérationnelle aide décision centre classes préparatoires grandes ecoles marrakech lycée ibn timiya filière mathématiques physique obtention baccalauréat sciences mathématiques b experience professionnele stage enquête nationale conjoncture auprès ménages sein direction régionale haut commissariat plan marrakech competences statistique inférence statistique analyse régression anova séries temporelles analyse données economie finance econométrie microéconomie macroéconomie gestion portefeuilles economie entreprises comptabilité nationale comptabilité générale recherche opérationnelle programmation linéaire programmation non linéaire processus stochastiques chaines markov fiabilité ntenance contrôle qualité file attente théorie graphes gestion stock production simulation évènements discrets informatique python beautifulsoup pandas numpy sikit learn matplotlib… r spss matlab scilab programmation orienté objet java langage structure données scilab uml eviews html uml solveur excel gestion base données mysql vb vba excel institut nationale statistique economie appliquée date naissance langues • arabe lu écrit parlé • français lu écrit parlé • anglais lu écrit parlé projets certificats classification machine learning évaluer déterminants solvabilité banque python anova étudier effet âge taux participation bénévole après génération aléatoire données partir moyennes obtenues site insee python spss analyse prévision prix indice boursier bist0 utilisant séries chronologiques sous r projet gestion location voiture java projet gestion classe utilisant structures données coursera ibm data science professional certificate coursera access to web data university of michigan coursera applied data science with python specialization cours • • • • • • • • activités parascolaires • président club insea charity • membre club golden mean • membre club great debaters insea insea'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_cleaner(convert_pdf_to_txt('CV.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The test of KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A database for testing using 10 CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################################### forget about this\n",
    "\n",
    "# L=[]\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\info (1).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\info (2).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\info (3).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\info (4).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\info (5).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\info (6).pdf\")))\n",
    "\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\comm (1).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\comm (2).pdf\")))\n",
    "\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\marketing (1).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\marketing (2).pdf\")))\n",
    "\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\mecanique.pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\mecanique (1).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\mecanique (2).pdf\")))\n",
    "# L.append(cv_cleaner(convert_pdf_to_txt(\"some_CVs\\mecanique (3).pdf\")))\n",
    "\n",
    "\n",
    "# len(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categories_numbers=['informatique','Marketing','Mecanique']\n",
    "\n",
    "# # D=6*['informatique']+4*['Marketing']+4*['Mecanique']\n",
    "# Y_train=np.array(6*[0]+4*[1]+4*[2])\n",
    "# Y_train\n",
    "# type(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Real Database and creating a new col for the clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV</th>\n",
       "      <th>Domaine</th>\n",
       "      <th>Texte_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv651</td>\n",
       "      <td>Informatique</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv255</td>\n",
       "      <td>Informatique</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv592</td>\n",
       "      <td>Informatique</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv593</td>\n",
       "      <td>Informatique</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv594</td>\n",
       "      <td>Informatique</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CV       Domaine Texte_CV\n",
       "0  cv651  Informatique         \n",
       "1  cv255  Informatique         \n",
       "2  cv592  Informatique         \n",
       "3  cv593  Informatique         \n",
       "4  cv594  Informatique         "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data=pd.read_excel(\"donnees.xlsx\")\n",
    "data['Texte_CV']=' '\n",
    "data.head()\n",
    "\n",
    "# intialise data of lists. \n",
    "# data = {'CV':L, 'Domaine':D} \n",
    "  \n",
    "# Create DataFrame \n",
    "# df = pd.DataFrame(data) \n",
    "  \n",
    "# Print the output. \n",
    "#print(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning each CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV</th>\n",
       "      <th>Domaine</th>\n",
       "      <th>Texte_CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv651</td>\n",
       "      <td>Informatique</td>\n",
       "      <td>ingénieur développement java jee api micro ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv255</td>\n",
       "      <td>Informatique</td>\n",
       "      <td>plus informations bienvenue www zakariadalil c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv592</td>\n",
       "      <td>Informatique</td>\n",
       "      <td>né adresse bardo tunisie tel el linked mahfoud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv593</td>\n",
       "      <td>Informatique</td>\n",
       "      <td>1 rue pasteur bourg reine cid 6 cid cid gpg id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv594</td>\n",
       "      <td>Informatique</td>\n",
       "      <td>h benjrad ans célibataire el ingénieur a école...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CV       Domaine                                           Texte_CV\n",
       "0  cv651  Informatique  ingénieur développement java jee api micro ser...\n",
       "1  cv255  Informatique  plus informations bienvenue www zakariadalil c...\n",
       "2  cv592  Informatique  né adresse bardo tunisie tel el linked mahfoud...\n",
       "3  cv593  Informatique  1 rue pasteur bourg reine cid 6 cid cid gpg id...\n",
       "4  cv594  Informatique  h benjrad ans célibataire el ingénieur a école..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,len(data)):\n",
    "    data['Texte_CV'][i]=cv_cleaner(convert_pdf_to_txt(\"C:\\\\Users\\\\Anonymous\\\\Desktop\\\\data_cv_training\\\\\"+data['CV'][i]+\".pdf\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure that the X input of your KNN model should be an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['Texte_CV'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the X to a numerical vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################### for test only\n",
    "\n",
    "# # Builds a dictionary of features and transforms documents to feature vectors and convert our text documents to a\n",
    "# # matrix of token counts (CountVectorizer)\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(L)\n",
    "# # transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary of features and transforms documents to feature vectors and convert our text documents to a\n",
    "# matrix of token counts (CountVectorizer)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data['Texte_CV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 7720)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Y also should be numerical value, so we will assign to each value a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Informatique', 'Logistique', 'Mecanique', 'Economie', 'Commerce',\n",
       "       'Juridique', 'RH'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Domaine'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 3, 3, 3, 3, 4, 4, 4, 5, 5, 2, 0, 0, 4, 4, 4, 4, 5, 5, 5, 5,\n",
       "       6, 4, 5, 5, 5, 5, 5, 5, 2, 2, 3, 4, 6, 1, 1, 1, 1, 4, 2, 2, 2, 2,\n",
       "       2, 2, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categories_numbers=['Informatique', 'Logistique', 'Mecanique', 'Economie', 'Commerce','Juridique', 'RH']\n",
    "#create new col for number o each category\n",
    "data['Domaine_number']=' '\n",
    "for i in range(0,len(data)):\n",
    "    data['Domaine_number'][i]=Categories_numbers.index(data['Domaine'][i])\n",
    "data.tail()\n",
    "# D=6*['informatique']+4*['Marketing']+4*['Mecanique']\n",
    "#be careful, it should be a numpy.int type not o native int\n",
    "Y_train=np.array(np.int32(data['Domaine_number'].values))\n",
    "# Y_train.shape\n",
    "type(Y_train[0])\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### difference between int and native int ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(np.int32(12))\n",
    "# Y_train=np.array(np.int32(data['Domaine_number'][1])\n",
    "Y=np.array(np.int32([1]*73))\n",
    "type(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "clf = knn.fit(X_train_tfidf,Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict some CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Marketing est un domaine tres communicant' => Commerce\n",
      "'Java est mecanique programme bon' => Informatique\n",
      "'projet mba management sport alternance win sport school o p é e e capacité apprentge rigoureux dynamique sérieux autonome orgé capacité triller équipe capacité encadrer former e x p é r i e e p r o f e i o e e service civique cjb handball entraînements participation stages perfectionnement présence weekends compétitions développement filière féminine intersport reze oceane vendeur chaussures sports collectifs conseil client mise rayon implantation rayon traitement produit inté leclerc drive atlantis réapprovisionnement racks livraison client préparation commande formation nouveaux intéaires f o r a i o ufr staps nantes licence 3 formation cours spécité education motricité obtention deug staps licence 2 équivalant bafa lycée colinière nantes bac stmg mention assez bien spécité gestion finance interne durant toute période lycée o a h a e e u i a h a b a e u r e h a u i v e a u p a i o é e p o r o b e i f p r o f e i o e développer réseau profession devenir employable fin études done lequel épanouis sens bien autrement dit done sport o o r o é e rue île sein sainte loire i f o r a i q u e maîtrise pack microsoft office davinci resolve kinovea logiciel montage vidéo audacity montage audio photoscape siire photoshop a g u e anglais niveau bac 2 espagnol niveau bac' => Commerce\n"
     ]
    }
   ],
   "source": [
    "# Input Data to predict their classes of the given categories\n",
    "docs_new = ['Marketing est un domaine tres communicant', 'Java est mecanique programme bon',cv_cleaner(convert_pdf_to_txt(\"cv211.pdf\"))]\n",
    "# building up feature vector of our input\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "# We call transform instead of fit_transform because it's already been fit\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, Categories_numbers[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code will read the test BD, create a colomn for the CV text, clean the CV, than vectorize the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it's so important that u dont create a new object(for transforming anf tf-idf ), u should work with the same one,,,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 1, 1, 2, 2, 5, 5, 4, 4, 3, 3, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load\n",
    "test_data=pd.read_excel(\"test_BD.xlsx\")\n",
    "test_data['Texte_CV']=' '\n",
    "#data.head()\n",
    "\n",
    "#clean\n",
    "for i in range(0,len(test_data)):\n",
    "    test_data['Texte_CV'][i]=cv_cleaner(convert_pdf_to_txt(\"C:\\\\Users\\\\Anonymous\\\\Desktop\\\\data_cv_training\\\\\"+test_data['CV'][i]+\".pdf\"))\n",
    "test_data.head()\n",
    "\n",
    "#vectorize tf-idf\n",
    "\n",
    "######################### it's so important that u dont create a new object, u should work with the same one,,,,,\n",
    "\n",
    "#count_vect = CountVectorizer()\n",
    "X_test_counts = count_vect.transform(test_data['Texte_CV'])\n",
    "# transform a count matrix to a normalized tf-idf representation (tf-idf transformer)\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "X_test_tfidf.shape\n",
    "\n",
    "\n",
    "\n",
    "#create new col for number o each category\n",
    "test_data['Domaine_number']=' '\n",
    "for i in range(0,len(test_data)):\n",
    "    test_data['Domaine_number'][i]=Categories_numbers.index(test_data['Domaine'][i])\n",
    "data.tail()\n",
    "\n",
    "#be careful, it should be a numpy.int type not o native int\n",
    "Y_test=np.array(np.int32(test_data['Domaine_number'].values))\n",
    "# Y_train.shape\n",
    "#type(Y_test[0])\n",
    "Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 7720)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will predict the \"Domaines\" of each CV in data_test, and than we will calculate the mean of true predicted one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got an accuracy of 86.66666666666667 % over the test data.\n"
     ]
    }
   ],
   "source": [
    "# test_data = fetch_20newsgroups(subset='test',\n",
    "#                                categories=categories, shuffle=True, random_state=42)\n",
    "# docs_test = test_data.data\n",
    "\n",
    "# Predicting our test data\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "print('We got an accuracy of',np.mean(predicted == Y_test)*100, '% over the test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 1, 1, 2, 0, 5, 5, 4, 4, 3, 6, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(X_test_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 1, 1, 2, 2, 5, 5, 4, 4, 3, 3, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the optimal K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "L=np.arange(1,6)\n",
    "L=np.zeros(4)\n",
    "L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example lets try all those value of K\n",
    "Ks=int(len(data)/2)\n",
    "# Ks=len(data)\n",
    "#np array to store accuracy\n",
    "accuracy=np.zeros(Ks-1)\n",
    "\n",
    "for k in range(1,Ks):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # training our classifier ; train_data.target will be having numbers assigned for each category in train data\n",
    "    clf = knn.fit(X_train_tfidf,Y_train)\n",
    "    # Predicting our test data\n",
    "    predicted = clf.predict(X_test_tfidf)\n",
    "#   print('We got an accuracy of',np.mean(predicted == Y_test)*100, '% over the test data.')\n",
    "    accuracy[k-1]=np.mean(predicted == Y_test)*100\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "K=np.arange(1,Ks)\n",
    "plt.plot(K,accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.66666666666667\n",
      "La valeur optimale de K est : 7\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.max())\n",
    "print('La valeur optimale de K est :',accuracy.argmax()+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
